#---------- this is a test config file
#for parameter combinations, use ';' to separate parameters (because for dist_tp_list, ',' also used)
#used for seq2seq architecture
#05.18: add en_bidirection/en_attention to support Model2 (also train/infer separated)

#---------- location

root_dir                		data_sim_data_type_bin_siamese_seq2seq/
#root_dir                               #data_sim_data_type_bin_seq2seq

#data_label              		L50_TR10K_VLD2K #case_bin
data_label				L50_TR20K_VLD4K_dependent_si_pairs

model_label                             siamese_seq2seq_TryNewModel_RL_PGPE
#model_label				siamese_seq2seq_TryNewModel #BiLSTM/Attention supported; also train/infer separated
#model_label                             siamese_seq2seq_LSTM
#model_label             		seq2seq_LSTM #init

#---------- general

seq_type                	        0	
architecture				2	#0: siamese (enc) and 1: seq2seq and 2: siamese seq2seq

#---------- data (clusters)

n_clusters				5000   #test: 2000
cluster_len				50

seq2seq_type				0	#0: cgk

si_correlation_type			1	#for siamese seq2seq architecture. 0: (si_1, si_2) independent. 1: si_2 is si_1 through an indel channel

#---------- data (si_2 independent)

cluster_len2				50      #for siamese seq2seq architecture. si_2 length, only used for si_2 that's independent of si_1

#---------- data (si_2 via indel channel)

rate_ins                                0.00    #only used when si_correlation_type is 1
rate_del                                0.00
rate_sub                                0.10

#---------- data (validation)

n_clusters_validation                   2000    #if -1 or not training (e.g. testing), validation data not generated for seq2seq

#---------- train (device)

allow_soft_placement			True
log_device_placement			True

#---------- train

learning_rate				0.001
grad_max				5.0     #grad clipping

output_buffer_size		        200000  #about batch_size * 1000
random_seed				0	

n_epoch                                 5
batch_size				100     #10000
n_batches                               500     #n_epoch * train_samples / batch_size; currently dummy

#---------- neural network model (input)

blocklen                                5 #1
embed_size				100

#---------- neural network model (encoder)

en_bidirection                          1 #1 use bi-directional rnn. per direction has enc_num_layers

enc_num_layers                          1 #1
enc_num_units                           150 #if en_bidirection==1, per fwd/bwd layer has half of enc_num_units
enc_forget_bias                         0.5                        

#---------- neural network model (decoder)

en_attention				1

dec_num_layers                          1 #1
dec_num_units				150
dec_forget_bias				0.5

#---------- Reinforcement Setup

RL_method                               

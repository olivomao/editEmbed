# tag TBD indicates things to be improved in the codes
# for example, embedding_cgk.py #load, MAXLEN, transform deprecated
#
# debug tip: fix rand seed ==> check np.isnan(loss) ==> before this iteration, enable tfdbg and check tensors w/ inf or nan

2018.06.02

summary: add support codes for pretrain inside siamese seq2seq training

batch_test.py
- for siamese seq2seq training, add pretrain related params
- for siamese seq2seq data, add pretrain related generation (siamese_seq2seq_data_pretrain)

storage_structure.txt
- add pretrain related data locations for siamese seq2seq

train.py
- add support of pretrain for siamese seq2seq training
  -- esp there're two functions mainly implemented: prepare_tf_dataset_input_files_pretrain and pretrain_siamese_seq2seq
  -- pretrain related codes appear where the apply_pre_train shows up 

2018.05.18

summary: support deviation_logger for train and validation. wrapup some functions for reuse and RL purpose in train_siamese_seq2seq

batch_test.py and storage_structure.txt
- add deviation_logger_vld path (to dump log for validation data)

train.py
packed train_siamese_seq2seq code by following structure/functions:
- SNA_ED_StateAction
- state2action
- Metrics 
- calc_metrics
 


2018.05.18

summary: apply Model2 (separate train/infer; support BiLSTM and attention) for train_siamese_seq2seq

logger.py
- add exception handling in case dev_vals has nan or inf

train.py
- move check_variables outside to be used by train_siamese_seq2seq
- in train_siamese_seq2seq, replace Model with Model2 (separate train/infer; support BiLSTM and attention)


2018.05.17-05.18

summary: add Model2 to separate train/infer data processing; bidirection and attention support for Model2

model_enc_dec.py
- attention mechanism (on-going)
- add bidirectional support (to be deprecated/ encoder_state is not consistent with decoder cell/ remedied at model_enc_dec_2.py)

model_enc_dec_2.py
- add this file to separate train/infer modes. parameters will be shared outside the model definition.
- add bi-directional support for encoder
  note:
  enc_num_layers will be applied to fwd and bwd;
  per layer per fwd bwd layer size is half of enc_layer_size
 
  bi_encoder_state = (fwd_out_state, bwd_out_state) where
  per out_state = (c,h) (one layer) or ((c0,h0),(c1,h1)...) (multiple layers)
  where c and h has shape (bs, hs)

  fwd_out_state and bwd_out_state need to be merged in terms of c and h
  in order to be compatible to decoder cell state input.
- add attention option
  unlike previous condition in model_enc_dec.py, both train/validation data is working

train.py
- train_seq2seq replace Model with Model2 (so we have model_train and model_infer to handle train and validation data separatedly) 

2018.05.16

summary: plot dev or dH to log file (via logger.py)

logger.py
- new file to organize logging operations
- class inheritance (abstract etc) added for future extension
- plot figure also added

batch_test.py
- add deviation_logger_path

file_structure.txt
- add dev_log format

model_enc_dec.py
- print init model name

storage_structure.txt
- add dev_log format

train.py
- enable shuffle
- add init model name
- set dev_de_d_nn nan to 0


2018.05.15

summary: add support of s_i_type into train pipeline

storage_structure.txt
- s_i_type

train.py
- prepare_data_siamese_seq2seq add s_i_type
- SiameseSeq2Seq_BatchedInput add s_i_type
- prepare_minibatch_seq2seq add s_i_type
- train_siamese_seq2seq add s_i_type

2018.05.15

summary: add support for dependant (si_1, si_2) where si_2 is obtained from si_1 through an indel channel

batch_test.py
- add ins/del/sub for batch_test_data

file_structure.txt
- add s_i_type for siamese_seq2seq format

indel_channel.py
- wrap sim_seq

simulate_data.py
- gen_siamese_seq2seq: explicit input args; 
                       support both independent and dependent (indel channel) (si_1, si_2) pairs

2018.05.15

summary: add data pipeline and train framework for siamese_seq2seq

batch_test.py
- add batch_test_train_1job_siamese_seq2seq

edit_dist.py
- use absolute edit distance (unnormalized)

file_structure.txt
- add siamese_seq2seq format

simulate_data.py
- gen_siamese_seq2seq
- proj_hamming_dist add an option for normalize

storage_structure.txt
- add siamese_seq2seq storage

train.py
- add prepare_data_siamese_seq2seq
- add SiameseSeq2Seq_BatchedInput
- add train_siamese_seq2seq
- add show_hist


2018.05.11

summary: add validation support for training seq2seq architecture

file_structure.txt
- add loss_log format for seq2seq training

model_enc_dec.py
- make the code cleaner and support both train and infer

train.py
- add plot_log2 for log dump during seq2seq training
- modify prepare_minibatch_seq2seq to support infer batch size
- add calc_validation_loss to calculate avg hamming loss b/w ref_seq_ids and predicted_seq_ids with shape=(batch size, time length) (they may have different time length)
- refine the train_seq2seq loop

2018.05.09

batch_test:
- train seq2seq

file_structure:
- add seq2seq_pair, seq2ids, vocab formats

inference:
- tried atention mechanism (currently disabled)

simulate_data:
- gen_seq2seq

storage_structure
- add seq2seq related files

train
- add seq2seq train related codes

model_enc_dec.py
- new seq2seq model

2018.04.10


summary: add validation loss. shuffle train and validation data. dump loss curves.

file_structure.txt
- add loss_log format (to dump training loss and validation loss fig)

storage_structure.txt
- add log.txt and log.png for training

proc_data.py
- shuffle x1,x2,y
- split_train_validation to provide validation data


train.py
- support plot loss (training and validation)
- also calculate validation

2018.04.04

summary: complete batch_test_eval (esp select and debug ckpt)

batch_test.py
- use ';' to seperate param combinations
- add select_ckpt (for a set of params, there can be several ckpt)

inference.py
- norm path for Predict's model_prefix, to avoid error when restoring saved tf model


2018.03.30

summary: set rand seed 0, add tfdbg option and do gradient clipping

inference.py
- siamese init: rand seed 0 (for debug purpose and reproducibility)

proc_data.py
- load2: rand seed 0 (for debug purpose and reproducibility)

train.py
- add debug option (tfdbg before sess.run and check loss nan)
- grad to be clipped by norm

2018.03.22

summary: apply normalization on edit dist and lstm layer output. balance number of different types of pairs.

edit_dist.py
- use normed dist

evaluation.py
- fix a bug for draw_histogram when there's only one dist metric (only one subplot)

inference.py
- hidden layer output normalized

proc_data.py
- rand sample type 1 pairs so that number of type 0 and type 1 pairs are similar.

util.py
- fix a bug for iterCounter.inc() (to avoid zero division)

2018.03.21

summary: support cgk embedding

simulate_data.py
- add support for cgk embedding (both binary and dna seq; rand seq same for both seqs of a pair)

2018.03.21

summary: support export_embed. e.g. x==>f(x)

evaluation.py
- add export_embedding

file_structure.txt
- add embed_fa

inference.py
- siamese: name output1 and output2 so that embedding can be extracted
- Predict: add get_embed method

test_sim_data_eval.sh
- add export_embedding related script


2018.03.21

summary:
add code comments

train.py
- prepare_data: remove tf.FLAGS

simulate_data.py
- add comments

proc_data.py
- delete transform and load2 (w/o seq2nn)
- add code comments

inference.py
- remove predict(...
- add code comments

2018.03.20

evaluation.py
- handle dist==nan issue
- add code comments

2018.03.19

summary:
- improve whole framework: consistent pipeline; support nn prediction and downstream analysis (histogram and roc); use seq2nn to handle data transform for both train and inference etc

train.py
- add prepare data (not using tf.FLAGS)
- better console interface

simulate_data.py
- pairwise dist file supports multiple dist types (e.g. calc_non_learned_dist)
- calc_dist_1thread: use seq2nn; pt to set max_num_dist_1thread for quick verification of code flow
- calc_dist_Nthread: related changes based on calc_dist_1thread

proc_data.py
- disable transform (e.g. global variables usage)

inference.py
- maxlen and blocklen no longer global variables; to be saved into model and restored later
- add Predict, seq2nn for prediction based on trained model
- add Fa_tseq class for prediction based on trained model
- load: apply seq2nn
- load2: use args and seq2nn

2018.03.08

indel_channel.py
- support bin/dna seq
- support calc num of sampled seq per cluster by tot_num*weigt or fixed copy_num

simulate_data.py
- weight normalized
- sample from cluster using 1 thread or multiple threads

util.py
- add run_cmds
- add split_fa_file and merge_files
